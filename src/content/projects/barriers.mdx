---
name: "barriers"
description: "Implementation of different scalable barrier algorithms which synchronize execution between multiple thread and machines. Written in C using OpenMP and MPI."
thumbnail:
  image: "../../assets/covers/barriers-cover.png"
  alt: "gRPC logo"
---

import { Image } from "astro:assets";

import treeImg from "../../assets/images/barriers/tree.png";
import tree2Img from "../../assets/images/barriers/tree-2.png";
import ompImg from "../../assets/images/barriers/omp.png";
import disseminationImg from "../../assets/images/barriers/dissemination.png";
import mpiImg from "../../assets/images/barriers/mpi.png";

## Overview

Coordinating the execution of multiple threads often requires efficient implementation of synchronization primitives provided by the Operating System.

As an operating system and library designer, how can we implement some of these primitives to scale efficiently on many threads?

How can we minimize latency and contention?

Can efficient synchronization be done not just across threads, but also multiple machines?

These are the questions I tried to answer in a project I completed in [Advanced Operating Systems](https://omscs.gatech.edu/cs-6210-advanced-operating-systems) at Georgia Tech where I implemented and compared various barrier algorithms discussed in the seminal paper,

> Mellor-Crummey, J. M., & Scott, M. L. (1991). _Algorithms for Scalable Synchronization on Shared-Memory Multiprocessor_. Computer Systems.
> https://dl.acm.org/doi/10.1145/103727.103729

## What are barriers?

A barrier is a synchronization primitive where all threads must enter the barrier before any thread can exit and continue executing.

Barriers are important in programs that need to synchronize stages of computation among threads.
For example, if multiple threads are parallelizing a discrete time simulation, you might want to ensure the state of the world in the previous time step is fully computed before you simulate the current time step.

### Counting Barrier

Counting barrier is a simple implementation that relies on multiple threads busy spinning on a shared memory location.
Busy spinning refers to a hot loop that continuously check on a condition in an infinite while loop.
The last thread to enter a barrier signals to all other threads by changing the shared memory.

**Advantages**: Simple, fewest assembly instructions.

**Disadvantages**: In cache coherent CPUs, each thread will cache the shared memory location it is spinning on in its L1 cache.
A [thundering herd](https://en.wikipedia.org/wiki/Thundering_herd_problem) then occurs when the shared location is changed and all threads invalidate their cache row and refetch the updated value from the core who cache line is in the modified state.
The amount of concurrent bus traffic scales as number of threads increase. This has the potential to stall the CPU and reduces the overall efficiency of the barrier.

### MCS Tree Barrier

<Image
  src={treeImg}
  alt="Tree based barriers"
  width="500"
  class="m-auto mb-4 mt-4"
/>

<Image
  src={tree2Img}
  alt="Tree based barriers"
  width="500"
  class="m-auto mb-4 mt-4"
/>

MCS proposes a solution that alleviates memory contention by distributing the spin variable in a tree based structure.

The tree structure is represented as an array of interconnected nodes. Quite uniquely, the structure encodes both a 4-ary arrival tree and a binary wake up tree all in a single array of node structs.

When a threads arrives at the barrier, it first spins until all of its children have arrived. Then it signals to its parent in the arrival tree. Then, it spins until the parent in the wake up tree signals to continue execution. Finally, it signal its children to continue and leaves the barrier.

**Advantages**: Avoids single spin location for all threads.

**Disadvantages**: Code is more complex.

### Which one is better?

Like most engineering decisions, it somewhat depends!

Using OpenMP to spin up multiple threads, we can compare the runtime of the two barriers.

```c
    #pragma omp parallel shared(num_threads)
    {
        struct timespec start, end;
        clock_gettime(CLOCK_MONOTONIC, &start);

        for (int i = 0; i < 1000000; i++)
        {
            barrier();
        }

        clock_gettime(CLOCK_MONOTONIC, &end);

        printf(
            "1000000 iterations took about %.5f seconds\n",
            ((double)end.tv_sec + 1.0e-9 * end.tv_nsec) -
            ((double)start.tv_sec + 1.0e-9 * start.tv_nsec)
        );
    }
```

<Image
  src={ompImg}
  alt="Results of shared memory barriers"
  width="700"
  class="m-auto mb-4 mt-4"
/>

The graph above shows the average latency per iteration of the barrier.

As expected, the MSC tree barrier scaled better with the number of threads due to less contention on the bus.
However, the counting barrier outperforms the tree barrier at low number of threads due to the simpler code.
Often times, asymptotic behavior is not the only consideration. Constant factor and problem size matter!

## MPI Barriers

To synchronize across machines, we must leverage a different mechanism of communication as shared memory and atomic operations are no longer possible.

[MPI](https://www.open-mpi.org/) is an API for expressing explicit network communication between compute nodes.
It is often used in scientific computing applications that run on many independent nodes of a supercomputing cluster.

For the project, I have implemented a **tournament barrier** and a **dissemination barrier** using `OpenMPI`.

Both of these algorithms does `O(log(N))` rounds of coordinated communication between nodes to reach consensus that everyone has reached the barrier.

<Image
  src={disseminationImg}
  alt="Dissemination Barrier"
  width="700"
  class="m-auto mb-4 mt-4"
/>

But like the shared memory barriers, Big O behavior does not tell the full story.
The tournament barrier is actually **twice** as slow as the dissemination barrier due to a constant factor difference in the number of serial communication rounds required.

<Image
  src={mpiImg}
  alt="Dissemination Barrier"
  width="700"
  class="m-auto mb-4 mt-4"
/>
